{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('data_postings.txt','r')\n",
    "postings = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('data_labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data we will never use until the end of the project\n",
    "post_valid, labels_valid = postings[10000:], labels[10000:]\n",
    "\n",
    "# data that we will use\n",
    "postings, labels = postings[:10000], labels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526, 526, 10000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_valid), len(labels_valid), len(postings), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics or a related field5+ years’ experience of Data platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Kinesis/Kafka/Spark/Storm implementations.Experience with analytic solutions applied to the Marketing or Risk needs of enterprisesBasic understanding of machine learning fundamentals.Ability to take Machine Learning models and implement them as part of data pipeline5+ years of IT platform implementation experience.Experience with one or more relevant tools ( Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis ).Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc).Current hands-on implementation experience requiredAt Amazon Web Services (AWS), we’re hiring highly technical cloud computing architects and engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.In this role, you will work with our partners, customers and focus on our AWS Analytics and ML service offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.In our Global Specialist Practice, you will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers, and, most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.Excellent business and communication skills are a must to develop and define key business questions and to build data sets that answer those questions. You should be able to work with business customers in understanding the business requirements and implementing solutions.This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.Masters or PhD in Computer Science, Physics, Engineering or Math.Hands on experience working on large-scale data science/data analytics projects.Ability to lead effectively across organizations.Hands-on experience with Data Analytics technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.Implementing AWS services in a variety of distributed computing, enterprise environments.Proficiency with at least one the languages such as C++, Java, Scala or Python.Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.Experience defining system architectures and exploring technical feasibility trade-offs.3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.Experience working on a code base with many contributors.Ability to prototype and evaluate applications and interaction methodologies.Experience with AWS technology stack.Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.,Data & ML Engineer - Nationwide Opportunities\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels \t: \tpostings\n",
      "\n",
      "DS\t:\t\"Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, ...\n",
      "SE\t:\t\"Bachelor’s degree or higher in Computer Science, Computer Engineering, Electric...\n",
      "SE\t:\t\"Purpose of the Role:The mission for this role is to follow established best eng...\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + postings[i][:80] + \"...\")\n",
    "    \n",
    "print(\"labels \\t: \\tpostings\\n\")\n",
    "pretty_print_review_and_label(0)\n",
    "pretty_print_review_and_label(1)\n",
    "pretty_print_review_and_label(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding correlations and removing meaningless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_counts = Counter()\n",
    "ds_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posting in range(len(postings)):\n",
    "    if labels[posting] == 'SE':\n",
    "        for word in postings[posting].split(\" \"):\n",
    "            se_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else:\n",
    "        for word in postings[posting].split(\" \"):\n",
    "            ds_counts[word] += 1\n",
    "            total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 146581),\n",
       " ('to', 79877),\n",
       " ('the', 64847),\n",
       " ('of', 61033),\n",
       " ('a', 46998),\n",
       " ('in', 45100),\n",
       " ('with', 40955),\n",
       " ('for', 29800),\n",
       " ('or', 25571),\n",
       " ('is', 21473)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the counts of the most common words in se_counts\n",
    "se_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 172711),\n",
       " ('to', 93667),\n",
       " ('the', 72643),\n",
       " ('of', 71550),\n",
       " ('in', 51420),\n",
       " ('a', 48498),\n",
       " ('with', 42729),\n",
       " ('data', 35041),\n",
       " ('for', 32343),\n",
       " ('or', 30099)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the counts of the most common words in ds_counts\n",
    "ds_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_ds_ratios = Counter()\n",
    "\n",
    "for term, count in list(total_counts.most_common()):\n",
    "    if count > 100:\n",
    "        se_ds_ratio = se_counts[term] / float(ds_counts[term]+1)\n",
    "        se_ds_ratios[term] = se_ds_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se-to-ds ratio for 'and' = 0.8487018852193247\n",
      "se-to-ds ratio for 'to' = 0.8527672203954392\n",
      "se-to-ds ratio for 'analytical' = 0.213768115942029\n",
      "se-to-ds ratio for 'Software' = 11.8188202247191\n",
      "se-to-ds ratio for 'Statistics' = 0.009174311926605505\n"
     ]
    }
   ],
   "source": [
    "print(\"se-to-ds ratio for 'and' = {}\".format(se_ds_ratios[\"and\"]))\n",
    "print(\"se-to-ds ratio for 'to' = {}\".format(se_ds_ratios[\"to\"]))\n",
    "print(\"se-to-ds ratio for 'analytical' = {}\".format(se_ds_ratios[\"analytical\"]))\n",
    "print(\"se-to-ds ratio for 'Software' = {}\".format(se_ds_ratios[\"Software\"]))\n",
    "print(\"se-to-ds ratio for 'Statistics' = {}\".format(se_ds_ratios[\"Statistics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, ratio in list(se_ds_ratios.most_common()):\n",
    "    if(ratio > 2):\n",
    "        se_ds_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        se_ds_ratios[word] = -np.log(1 / (ratio+0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se-to-ds ratio for 'and' = -0.15233346587193464\n",
      "se-to-ds ratio for 'to' = -0.14761035731282415\n",
      "se-to-ds ratio for 'analytical' = -1.4971449600006292\n",
      "se-to-ds ratio for 'Software' = 2.4696931952129306\n",
      "se-to-ds ratio for 'Statistics' = -3.954183816252424\n"
     ]
    }
   ],
   "source": [
    "print(\"se-to-ds ratio for 'and' = {}\".format(se_ds_ratios[\"and\"]))\n",
    "print(\"se-to-ds ratio for 'to' = {}\".format(se_ds_ratios[\"to\"]))\n",
    "print(\"se-to-ds ratio for 'analytical' = {}\".format(se_ds_ratios[\"analytical\"]))\n",
    "print(\"se-to-ds ratio for 'Software' = {}\".format(se_ds_ratios[\"Software\"]))\n",
    "print(\"se-to-ds ratio for 'Statistics' = {}\".format(se_ds_ratios[\"Statistics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tanzu', 5.25227342804663),\n",
       " ('Relic', 5.236441962829949),\n",
       " ('Cox', 5.017279836814924),\n",
       " ('Macy’s', 4.718498871295094),\n",
       " ('REST,', 4.363098624788363),\n",
       " ('observability', 4.02535169073515),\n",
       " ('.Net', 3.9186675481468147),\n",
       " ('Observability', 3.7534179752515073),\n",
       " ('Selenium', 3.506557897319982),\n",
       " ('.NET', 3.492169159867882),\n",
       " ('Android', 3.4790398684260895),\n",
       " ('test-driven', 3.3843902633457743),\n",
       " ('frontend', 3.332204510175204),\n",
       " ('TypeScript,', 3.3232358401924436),\n",
       " ('Node.js', 3.2088254890146994),\n",
       " ('Angular', 3.169685580677429),\n",
       " ('MVC,', 3.068052935133617),\n",
       " ('JavaScript', 3.057261463500853),\n",
       " ('Ruby', 3.0492730404820207),\n",
       " ('client-side', 2.9856819377004897),\n",
       " ('iOS', 2.9719216248602724),\n",
       " ('bug', 2.89591193827178),\n",
       " ('microservice', 2.820055259478705),\n",
       " ('CIS,', 2.816263785742443),\n",
       " ('C#', 2.739548868161581),\n",
       " ('React', 2.7365296724240786),\n",
       " ('HTML,', 2.719100037288795),\n",
       " ('pair', 2.719100037288795),\n",
       " ('JS', 2.70805020110221),\n",
       " ('Embedded', 2.633547673804289)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_ds_ratios.most_common()[:30] # 30 most important words for SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AstraZeneca', -4.605170185988092),\n",
       " ('hypotheses', -4.605170185988092),\n",
       " ('descriptive', -4.605170185988092),\n",
       " ('Biostatistics,', -4.605170185988092),\n",
       " ('ADaM', -4.605170185988092),\n",
       " ('SDTM', -4.605170185988092),\n",
       " ('CDISC', -4.605170185988092),\n",
       " ('GSK', -4.605170185988092),\n",
       " ('Statistician', -4.605170185988092),\n",
       " ('Bayesian', -4.605170185988092),\n",
       " ('Scientist,', -4.605170185988092),\n",
       " ('Economics,', -4.356369901080518),\n",
       " ('Scientist\"', -4.283444256995934),\n",
       " ('Scientist', -4.1339849679051675),\n",
       " ('statistical,', -4.109848748758066),\n",
       " ('\"Data', -4.072059517432666),\n",
       " ('semester', -3.991359747664667),\n",
       " ('Statistical', -3.9806176556315447),\n",
       " ('SPSS,', -3.9753926193607354),\n",
       " ('sentiment', -3.958543021063039),\n",
       " ('Statistics', -3.954183816252424),\n",
       " ('Analyst\"', -3.954183816252424),\n",
       " ('statistical', -3.8331932879237374),\n",
       " ('supervised', -3.7423380326387035),\n",
       " ('causal', -3.687376824229401),\n",
       " ('Statistics,', -3.646799368074074),\n",
       " ('Science\"\"\"', -3.619102576314776),\n",
       " ('unsupervised', -3.5631767913194894),\n",
       " ('Medicine', -3.506557897319982),\n",
       " ('clustering,', -3.5032134125971344)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(se_ds_ratios.most_common()))[0:30] # 30 most important words for DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tigra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#! pip install nltk\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    This function removes the noise from the job postings.\n",
    "    \n",
    "    INPUT: text data that has one posting per line\n",
    "    OUTPUT: text data that has one posting per line\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for line in range(len(data)):\n",
    "        word_list = ''\n",
    "        for word in data[line].split(\" \"):\n",
    "            if se_ds_ratios[word] > 1 or se_ds_ratios[word] < -1:\n",
    "                word_list += word + ' '\n",
    "        word_list = re.sub(r\"[^a-zA-Z0-9]\", \" \", word_list.lower())\n",
    "        word_list = list(set(word_tokenize(word_list)))\n",
    "        sentence = \" \".join(word_list)\n",
    "        new_data.append(sentence)\n",
    "    return new_data\n",
    "        \n",
    "new_data = preprocess_data(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kafka professional consultants analytics engineer models risk scientists learning tensorflow javascript code pytorch ml java software emr specialist physics sets applied sql answer scalability engagements phd analytic data mathematics web deep machine spark marketing'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DS'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models scientists c approaches iot learning algorithms mobile data ml novel environmental processing machine train embedded software engineer'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SE'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_train, labels_train = new_data[:7000], labels[:7000]\n",
    "post_test, labels_test = new_data[7000:], labels[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 3000, 526)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_train), len(post_test), len(post_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 3000, 526)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_train), len(labels_test), len(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a counter based on the training dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counter.fit(new_data)\n",
    "\n",
    "#count the number of times each term appears in a document and transform each doc into a count vector\n",
    "\n",
    "counts_train = counter.transform(post_train)#transform the training data\n",
    "counts_test = counter.transform(post_test)#transform the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#train classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "#train classifier on the same datasets\n",
    "clf.fit(counts_train,labels_train)\n",
    "\n",
    "#use hard voting to predict (majority voting)\n",
    "pred=clf.predict(counts_test)\n",
    "\n",
    "#print accuracy\n",
    "print (accuracy_score(pred,labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#train classifier\n",
    "KNN_classifier=KNeighborsClassifier()\n",
    "\n",
    "#train classifier on the same datasets\n",
    "KNN_classifier.fit(counts_train,labels_train)\n",
    "\n",
    "#use hard voting to predict (majority voting)\n",
    "pred=KNN_classifier.predict(counts_test)\n",
    "\n",
    "#print accuracy\n",
    "print (accuracy_score(pred,labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9581749049429658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counts_all = counter.fit_transform(new_data)\n",
    "\n",
    "def classify(data, labels):\n",
    "    \"\"\"\n",
    "    This function classifies unseen dataset.\n",
    "    \n",
    "    INPUT: list of job ostings where each element is a posting\n",
    "    OUTPUT: prints the accuracy of the classification\n",
    "    \"\"\"\n",
    "    \n",
    "    data_prep = preprocess_data(data)\n",
    "    cv_valid = CountVectorizer(vocabulary=counter.get_feature_names())\n",
    "    valid_data = cv_valid.fit_transform(data_prep)\n",
    "    pred = clf.predict(valid_data)\n",
    "    print(accuracy_score(pred,labels))\n",
    "    with open('pred_labels.txt', 'w', newline ='') as f:\n",
    "        # using csv.writer method from CSV package\n",
    "        write = csv.writer(f)\n",
    "        for i in pred:\n",
    "            write.writerows([[i]])\n",
    "    \n",
    "classify(post_valid, labels_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
